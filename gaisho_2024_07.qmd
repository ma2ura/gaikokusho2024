---
title: |
  <b>外国書購読 Day7</b> </br>
  <span style="color: #282A36; "> Chapter 6. <br> Analytical Review: Intelligence Scanning </span>
author: "Soichi Matsuura"
date: "20 Nov. 2024"
format:
  revealjs:
    theme: ["default", "dracula.scss"]
    transition: slide
    slide-number: true
    code-line-numbers: false
    html-math-method: katex
    chalkboard: true
    touch: true
    controls: true
    width: 1300
    # height: 900
    footer: "Kobe University, Business Administration"
    logo: "img/kobe_logo.png"
    include-in-header:
      text: |
        <style>
        .v-center-container {
          display: flex;
          justify-content: center;
          align-items: center;
        }
        </style>
execute:
  echo: true
  warning: false
  highlight-style: github
css: mystyle.css
---

# 本日の学習内容

1. HTML / CSS / XPath
2. Chapter 6. Analytical Review: Intelligence Scanning

講義の最初に，ウェブサイトの仕組みを簡単に説明し，ウェブ・スクレイピングのための基礎知識を整理します。

## packages

<!-- 必要なパッケージをインストールする -->
I use `pacman::p_load()` to install packages from CRAN and `pacman::p_load_gh()` from GitHub.

```{r}
pacman::p_load(tidyverse, kableExtra)
pacman::p_load(tidytext, textdata, htm2txt, tokenizers, wordcloud, rvest)
pacman::p_load_gh("mkearney/rtweet", "hrbrmstr/seymour","mkearney/newsAPI")
pacman::p_load(maps, tm, SnowballC, RColorBrewer, RCurl, stringr, httr, XML)
```

## 事前準備

ウェブスクレイピングについて学習する前に、必要な知識を整理します。

- HTML / CSS
- XPath

これらの知識があると、ウェブサイトから必要なデータを取得することができます。

## HTML

- **HTML**(HyperText Markup Language)は、Webページの内容を記述するためのマークアップ言語です。
- 基本的には、タグという記号を使って、テキストや画像などの要素を囲むことで、その要素の意味を示します。

```
<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <title>外国書購読2024</title>
</head>
<body>
<h1>見出し1</h1>
  ここに本文
</body>
</html>
```

## CSS

- **CSS**(Cascading Style Sheets)は、Webページの見た目を指定するための言語です。
- HTMLで記述された要素に対して、色や大きさ、位置などのスタイルを指定することができます。

```
h1 {
  color: blue;
  font-size: 24px;
}
```

とすると、`<h1>`タグで囲まれたテキストが青色になり、フォントサイズが24pxになります。


## XPath

- **XPath**は、XML文書の中から要素を選択するための言語です。
- XPathは、XML文書の構造を辿りながら、要素を指定することができます。
- XPathは、Webスクレイピングの際に、特定の要素を取得するために使用されます。
- XPathは、HTML文書にも適用することができます。

## HTMLの表

- HTMLの表は、`<table>`タグで囲まれた要素で構成されます。
- 表の行は`<tr>`タグで、列は`<td>`タグで囲まれます。
- 表の見出しは`<th>`タグで囲まれます。

```
<table id="namelist">
  <caption>
    表1: 名前のリスト
  </caption>
  <thead>
    <tr>
      <th>氏名</th><th>属性</th><th>年齢</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Chris</td><td>HTML tables</td><td>22</td>
    </tr>
    <tr>
      <td>Dennis</td><td>Web accessibility</td><td>45</td>
    </tr>
    <tr>
      <td>Sarah</td><td>JavaScript frameworks</td><td>29</td>
    </tr>
    <tr>
      <td>Karen</td><td>Web performance</td><td>36</td>
    </tr>
  </tbody>
</table>
```

## 表を抽出 | table

Wikipeidaのページから表を抽出します。

```{r}
url_akb <- "https://ja.wikipedia.org/wiki/AKB48"
AKB_table <- url_akb |> read_html() |> html_nodes("table") |> html_table(fill = T)
AKB_table[[9]] |> as_tibble() |>head(10) |>
 kable(format="html") |> kable_styling(font_size = 10)
```


## 表を抽出 | xpath

```{r}
url <- read_html("HTML/gaisho.html")
url |>html_node(xpath = '//*[@id="namelist"]') |>
  html_table(fill = T) |>
  as_tibble() |> kable()
```

ここで，`xpath = '//*[@id="namelist"]'`は，`id="namelist"`という属性を持つ要素を指定しています。



# Intelligence Scanning of Internet Resources
<!-- ## インターネット資源の知能スキャン -->



## Analytical Procedures

- Analytical procedures involve <mark class="mark">the study of plausible relationships between both financial and non-financial data</mark>.
- Before the mid-2000s, sources of such information were <mark class="mark">spotty, unreliable, and scarce</mark>, and that, indeed, was considered a prime reason for markets needing **annual, audited financial statements**.
- Today, there are <mark class="mark">numerous social networks, news, and discussion boards that offer both raw and curated, streaming sources of useful business intelligence</mark>.



:::{.notes}
分析的手続は，財務データと非財務データの両方の間に存在する妥当な関係の研究が含まれる。
2000年代中期以前は、このような情報ソースは断片的で、信頼性が低く、乏しかったため、市場が監査済み財務諸表を必要とする主な理由と考えられていた。
今日では、有用なビジネスインテリジェンスの生の情報源とキュレーションされたストリーミングソースの両方を提供する多数のソーシャルネットワーク、ニュース、およびディスカッションボードがある。
:::

---

- The auditor who does not scan for such client and industry intelligence both increases the cost of auditing, and can be considered **negligent** in not investigating all available information about the financial health of their client.
- Though this information is available, the raw information itself may need considerable interpretation and processing before an auditor may rely on it for audit work.
- <mark class="mark">The most useful information for analytical review may be qualitative and textual</mark>, so unsuitable for the sort of mathematical analysis offered in spreadsheet packages and statistical programs.





:::{.notes}
このような情報を利用できる一方で，監査作業に頼る前に、生の情報それ自体がかなりの解釈と処理を必要とする場合がある。
分析的レビュー(analytical review)に最も有用な情報が質的かつ文字である可能性もあるため、スプレッドシートパッケージや統計プログラムで提供される数学的分析には適していない場合がある。
このようなクライアントおよび業界のインテリジェンスを調べない監査人は、監査コストを増加させ、クライアントの財務健全性に関する利用可能なすべての情報を調査しないという点で、怠慢とみなされるかもしれない。
:::


---

- **Qualitative data** may be most useful when it is used to <mark class="mark">gauge sentiment of customers, investors, partners, employees, and vendors toward the company</mark>.
- It is only in the past decade that tools have become available to glean such intelligence from Internet streams and databases.
- This chapter provides a set of tools that will assist the auditor in interpreting and understanding <mark class="mark">the relevant qualitative data accessed through the Internet</mark>.


:::{.notes}
質的データは，顧客，投資家，パートナー，従業員，およびベンダーが企業に対してどのような感情を抱いているかを測定するために使用されるときに最も有用である。
この10年間で、インターネットのストリームやデータベースからそのような情報を収集するツールが利用可能となった。
この章では、インターネットを通じで入手した関連する質的データを監査人が解釈し、理解するのを支援する一連のツールを提供する。

:::


## Sentiment Analysis with Tidy Data

- I provide **algorithms** to map qualitative audit intelligence into consumer, investor, and vendor sentiment.
- I also suggest useful Internet resources for the audit, and provide algorithms for interpreting the scanned intelligence concerning the audit client, its competitors and the industry.
- When human readers approach a text, we use our understanding of <mark class="mark">the emotional intent of words</mark> to infer whether a section of text is positive or negative, or perhaps characterized by some other more nuanced emotion like surprise or disgust.
- We can use the tools of **text mining** to approach the emotional content of text programmatically.


:::{.notes}
本書では、質的監査情報を消費者や投資家、ベンダーの感情(sentiment)に写像するアルゴリズムを提供する。
また、監査のための有用なインターネットリソースを示唆し、監査クライアント、その競争相手、および業界に関するスキャンされた情報を解釈するためのアルゴリズムを提供する。
人間の読者がテキストにアプローチするとき、私たちは単語の感情的な意図を理解して、テキストのセクションがポジティブかネガティブか、または驚きや嫌悪など、もっと微妙な感情に特徴づけられているかどうかを推測する。
テキストの感情的な内容をプログラムでアプローチするために、テキストマイニングのツールを使用することができる。
:::


## Tidy Data

- We start by representing text in R’s **tidy** structure:
<!-- 私たちはRの**整然**(tidy)構造でテキストを表現することから始める： -->

- <mark class="mark">Each variable is a column</mark>.
- <mark class="mark">Each observation is a row</mark>.
- <mark class="mark">Each type of observational unit is a table</mark>.


<!--
- 各変数は列である。
- 各観察は行である。
- 各タイプの観察単位はテーブルである。
 -->


## 整然データとは (補足)


> “Tidy datasets are all alike, but every messy dataset is messy in its own way.”
> — Hadley Wickham


- R神Hadley Wickham氏は，データの型を理解することを，データ分析の第一歩とし，その一貫として**整然データ**(tidy data)という考え方を提唱しています。
- 整然データとは，次のようなルールに従って構築されたデータのことです(Wickham, 2014), 参考[https://id.fnshr.info/2017/01/09/tidy-data-intro/](https://id.fnshr.info/2017/01/09/tidy-data-intro/)。


<!--
1. Each variable is a column; each column is a variable.
2. Each observation is a row; each row is an observation.
3. Each value is a cell; each cell is a single value.
 -->

## 整然データのルール

:::{.callout-important}
1. 各観測値(observation)は行(row)であり、各行は1つの観測値である。
2. 各変数(variable)は列(column)であり、各列は1つの変数である。
3. 各値(value)はセル(cell)であり、1セルは1つの値である。
:::


:::{layout-ncol=3}
![1.観測値](img/observation.png)

![2.変数](img/variables.png)

![3.値](img/values.png)
:::

## 整然データの特徴と操作

整然データのルールを満たすデータは，データの整理や可視化が容易になります。
そして整然データを扱うために非常に強力なツールを提供してくれるのが，`tidyverse`パッケージ群です。
以下では，`tidyverse`パッケージ群の中でも，データの整理に特化した`tidyr`パッケージを使って，整然データを作成する方法を学びます。

```{r}
library(tidyr)
```

## long形式とwide形式

人間には読みやすいけれどパソコンは読みにくい，というデータの形式があります。例えば下の表を見てみましょう。

| 地点    |   2022    |   2023   |
|:------:|:--------:|:--------:|
| トヨタ自動車 | 31379507  | 37154298  |
| 日産自動車 | 8424585  |  10596695  |
| 本田技研工業 |  14552696  | 16907725  |
: 自動車会社3社の売上高

このような形のデータをワイド形式(wide)といいます。
この表は，人間にとっては分かりやすいですが，実はコンピュータにとっては，分かりにくいものです。
またこのデータは、列が変数になっていないので整然データではありません。

## Readalbe data

ロング型の表はこう。

| 企業名     |  年度 |  売上高     |
|:---------:|:----:|:-----------:|
| トヨタ自動車 | 2022  | 31379507    |
| トヨタ自動車 | 2023  | 37154298    |
| 日産自動車   | 2022  | 8424585    |
| 日産自動車   | 2023  | 10596695    |
| 本田技研工業 | 2022  | 14552696    |
| 本田技研工業 | 2023  | 16907725    |
: 自動車会社のロング型の表

このような形式のデータをロング型(long)といいます。


---

- We thus define the tidy text format as being a table with **one-token-per-row**.
- <mark class="mark">A token is a meaningful unit of text, such as a word, that we are interested in using for analysis, and tokenization is the process of splitting text into tokens</mark>.
- This one-token-per-row structure is in contrast to the ways text is often stored in current analyses, perhaps as strings or in a **document-term matrix**.


:::{.notes}
したがって、整然とした文字列フォーマットを、一行ごとに一つのトークン(one-token-per-row)を持つ表として定義する。
トークンとは、分析に利用することに関心のある、単語のような意味のある文字列の単位である。
トークン化(tokenization)とは、テキストをトークンに分割するプロセスである。
この一行ごとに一つのトークンの構造は、本書が分析の中でしばしば格納していた構造とは対照的で、おそらく文字列や文書用語行列となる。
:::

---

- For tidy text mining, the token that is stored in each row is most often a single word, but can also be an n-gram, sentence, or paragraph.

- R’s `tidytext` package provides functionality to tokenize by commonly used units of text like these and convert to a one-term-per-row format.


:::{.notes}
整然なテキストマイニングのために、各行に格納されるトークンは単一の単語だが、n-gram、文、または段落も可能である。
Rのtidytextパッケージは、これらのような一般的に使用されるテキストの単位をトークナイズし、一用語ごとの行形式に変換する機能を提供している。
:::

## Sentiment Analysis with Tidy Data

- One way to analyze the sentiment of a text is to consider <mark class="mark">the text as a combination of its individual words and the sentiment content of the whole text as the sum of the sentiment content of the individual words</mark>.
- This is not the only way to approach sentiment analysis, but it is an often-used approach, and an approach that naturally takes advantage of the tidy tool ecosystem.
- The `tidytext` package contains several **sentiment lexicons** in the `sentiments` dataset.


:::{.notes}
テキストの感情を分析する一つの方法は、テキストを個々の単語の組み合わせとして考え、全体のテキストの感情内容を個々の単語の感情内容の合計として考えることである。
これが感情分析にアプローチする唯一の方法というわけではないが、よく使用されるアプローチであり、整然なツールエコシステムの利点を自然に活用するアプローチといえる。
tidytextパッケージには，sentimentsデータセットにいくつかの感情辞書が含まれている。
:::

## Sentiment

For example, consider the following code chunk.
<!-- たとえば，次のコードチャンクを考えてみよう。 -->

```{r}
sentiments
```

## Sentiment Dictionary

The three general-purpose lexicons (辞典) are
<!-- 3つの汎用辞書は次のとおりです。 -->

- `bing` from Bing Liu and collaborators at University of Illinois—Chicago,
<!-- - bingはBing Liuとイリノイ大学シカゴ校の共同研究者によるもの -->
- `AFINN` from Finn Årup Nielsen, and
<!-- - AFINNはFinn Årup Nielsenによるもの -->
- `nrc` from Saif Mohammad and Peter Turney.
<!-- - nrcはSaif MohammadとPeter Turneyによるもの -->

---

- All three of these lexicons are based on **unigrams**, i.e., single words.
- These lexicons contain many English words and <mark class = "mark">the words are assigned scores for positive/negative sentiment</mark>, and also possibly emotions like joy, anger, sadness, and so forth.
- All three were constructed via either crowdsourcing (using, for example, Amazon Mechanical Turk) or by the labor of one of the authors, and were validated using some combination of crowdsourcing again, restaurant or movie reviews, or Twitter data.

:::{.notes}
これら3つの辞書はすべて、つまりユニグラム(つまり，単一の単語)で構成されている。
これらの辞書には多くの英単語が含まれており、その単語にはポジティブ／ネガティブな感情のスコアが割り当てられ、また喜び、怒り、悲しみなどの感情も可能性として含まれている。
これら3つはいずれも、クラウドソーシング（例えば、Amazon Mechanical Turkの使用）または著者の一人の労働によって構築され、クラウドソーシングを再び使用したり、レストランや映画のレビュー、またはTwitterのデータを使用して検証された。
:::

## lexicon

- The `nrc` lexicon categorizes words in a **binary fashion** (“yes”/“no”) into categories of <mark class="mark">positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust</mark>.
<!-- nrc語彙集は、ポジティブ、ネガティブ、怒り、期待、嫌悪、恐怖、喜び、悲しみ、驚き、信頼のカテゴリーに「はい」/「いいえ」の二項方式で単語を分類している。 -->
- The `bing` lexicon categorizes words in a **binary fashion** into positive and negative categories.
<!-- bing語彙集は、ポジティブとネガティブのカテゴリーに単語を二項方式で分類している。 -->
- The `AFINN` lexicon assigns words with a **score** that runs between $−5$ and $5$, with negative scores indicating negative sentiment and positive scores indicating positive sentiment.
<!-- AFINN語彙集は、−5から5までのスコアを単語に割り当て、ネガティブなスコアはネガティブな感情を、ポジティブなスコアはポジティブな感情を示している。 -->

## `afinn` dictionary

All of this information is tabulated in the sentiments dataset, and `tidytext` provides a function `get_sentiments()` to get specific sentiment lexicons without the columns that are not used in that lexicon.
<!-- このすべての情報は感情データセットに表形式でまとめられており、tidytextは特定の感情語彙集を、その語彙集で使用されていない列なしで取得するための関数`get_sentiments()`を提供している。 -->

```{r}
get_sentiments("afinn") # afinn辞書
```

## `bing` dictionary

```{r}
get_sentiments("bing")  # bing辞書
```

## `nrc` dictionary

```{r}
get_sentiments("nrc")   # nrc辞書
```

## Sentiment lexison

- There are also some <mark class="mark">domain-specific sentiment lexicons</mark> available, constructed to be used with text from a specific content area - e.g., for accounting and finance.
- Dictionary-based methods like the ones we are discussing find the total sentiment of a piece of text by adding up the individual sentiment scores for each word in the text.
- Not every English word is in the lexicons because many English words are pretty neutral.

:::{.notes}
特定の分野、例えば会計やファイナンスの教科書を使用するために構築された、ドメイン固有の感情語彙集もある。
本書で説明しているような辞書ベースの方法により、テキスト内の各単語の個別の感情スコアを合計することでテキストの総合的な感情が分かる。
多くの英単語がかなり中立的なため、すべての英単語が語彙集に含まれているわけではない。
:::

## 会計専門極性辞書

神奈川大学の平井先生らが作成した会計専門極性辞書

[会計専門極性辞書](https://mac-ie-kanagawa-u.studio.site/Dictionary)

- 平井裕久・小村亜唯子・川邊貴彬（2024）「トーン分析における会計専門極性辞書の利用可能性」『産業徑理』Vol.83, No.4, pp.41-50.

---


- It is important to keep in mind that these methods do not take into account qualifiers before a word, such as in “no good” or “not true”; a lexicon-based method like this is based on unigrams only.
- One last caveat is that the size of the chunk of text that we use to add up unigram sentiment scores can have an effect on results.
- A text the size of many paragraphs can often have positive and negative sentiment averaged out to about zero, while sentence-sized or paragraph-sized text often works better.


:::{.notes}
これらの方法は、「no good」や「not true」といった単語の前にある修飾語を考慮に入れないことを念頭に置くことが重要である。
このような語彙集ベースの方法はユニグラム(unigram)のみに基づいている。
最後の注意点として、ユニグラムの感情スコアを合計するために使用するテキストのチャンクのサイズが結果に影響を与える可能性がある。
多くのパラグラフのサイズは、しばしばポジティブとネガティブの感情が平均してゼロになることがありますが、文サイズや段落サイズのテキストの方がうまく機能することがよくあります。
:::


## Text mining

- With data in a tidy format, **sentiment analysis** can be done as an inner join.
- This is another of the great successes of viewing text mining as a tidy data analysis task; much as removing **stop words** is an antijoin operation, performing sentiment analysis is an <mark class="mark">inner join operation</mark>.
- Let us look at the words with a joy score from the NRC lexicon.
- For this example, we capture an HTML formatted General Motors’ 10-K report for 2017 from SEC’s ECGAR database, demonstrating that HTML documents may be used in the workpapers, as well as those in XBRL format.


:::{.notes}
データが整然データ形式であれば、感情分析は内部結合として実行できる。
これは、テキストマイニングを整然データ分析としてみなすことができる、というの大きな成功の一つである。
ストップワードの削除がアンチジョイン操作であるように、感情分析の実行は内部結合操作である。
NRC辞書からjoyスコアを持つ単語を見てみよう。
この例では、SECのECGARデータベースから2017年のHTMLフォーマットのGeneral Motorsの10-Kレポートをキャプチャし、XBRLフォーマットのものと同様にHTMLドキュメントが法律文書で使用される可能性があることを示している。
:::

## Word Cloud of Sentiment

```{r wordcloud1}
#| cache: true
#| output-location: slide
txt <- read_html("XBRL/gm201710k.htm", encoding = "UTF-8") |> html_text()
text_stuff <- txt |>
  tokenize_words() |> unlist() |> as_tibble()
colnames(text_stuff) <- "word"
stuff_sentiment <- text_stuff |>
  inner_join(get_sentiments("bing"), by = "word")

text_stuff |>
  anti_join(get_stopwords()) |>
  inner_join(stuff_sentiment) |>
  count(word) |>
  with(wordcloud(word, colors = rainbow(3), rot.per = 0.15, n, max.words = 1000))
```

<!--
txt <- read_html("XBRL/gm201710k.htm", encoding = "UTF-8") |> html_text()
text_stuff <- txt |>
  tokenize_words() |> unlist() |> as_tibble()
colnames(text_stuff) <- "word"
stuff_sentiment <- text_stuff |>
  inner_join(get_sentiments("bing"), by = "word")

text_stuff |>
  anti_join(get_stopwords()) |>
  inner_join(stuff_sentiment) |>
  count(word) |>
  with(wordcloud(word, colors = rainbow(3), rot.per = 0.15, n, max.words = 1000))


 -->

## Sentiment Analysis with Tidy Data

```{r}
#| output-location: slide
net_sentiment <- text_stuff |>
  inner_join(get_sentiments("bing"), by = "word") |>
  count(sentiment) |>
  spread(sentiment, n, fill = 0) |>
  mutate(
    net_positive = positive - negative,
    proportion_positive = positive/negative - 1
    )

net_sentiment |>
  kable() |>
  kable_styling(
    bootstrap_options = c("striped", "hover", "condensed")
    )
```

## text analysis

```{r}
#| cache: true
#| output-location: slide
text_stuff %>%
    anti_join(get_stopwords()) %>%
    inner_join(stuff_sentiment) %>%
    count(word) %>%
    with(wordcloud(
        word,
        colors = rainbow(3),
        rot.per = 0.15,
        n,
        max.words = 1000)
        )
```


# Scanning of Uncurated News Sources from Social Networks
<!-- ## ソーシャルネットワークからキュレートされていないニュース情報を調べる -->

## X.com

- X.com (the platform formerly known as Twitter, we will use the terms Twitter/tweets here) is arguably the most important of all the social networks for market analysis.
<!-- X.com (以前のTwitterとして知られているプラットフォーム)は、市場分析においておそらく最も重要なソーシャルネットワークである。 -->
<!-- - Twitter is arguably the most important of all the social networks for market analysis. -->
<!-- Twitterはおそらく全ソーシャルネットワークの中で市場分析とって最も重要なものである。 -->
- Facebook, MySpace, Instagram, Snapchat, LinkedIn, WeChat, and so forth potentially could offer more useful marketing information, but regulations either prohibit, the distribution of data completely (e.g., LinkedIn) or they significantly limit it to just they people you may know directly (Facebook).
<!-- FacebookやMySpace，Instagram，Snapchat，LinkedIn，WeChatなどは、より有用なマーケティング情報を提供する可能性があるが， -->
<!-- 規制によってデータの配布が完全に禁止されている（LinkedInなど）か、直接の知人だけのように大幅に制限されている（Facebookなど）。 -->
- This section shows you how to gain access to Twitter’s API (application programming interface).
<!-- この節では、TwitterのAPI（アプリケーションプログラミングインターフェース）にアクセスする方法を説明する。 -->

---

- X.com/Twitter has around 260 million daily active users as of 2023, a number that vastly dwarfs any of the finance specific social platforms (e.g., StockTwits) which have usage measured in hundreds of thousands.
<!-- Twitterは1日1億3000万人のアクティブユーザーを抱えており、これは数十万人単位で使用されている金融特化型のソーシャルプラットフォーム（StockTwitsなど）よりもはるかに多い。 -->
- Even though it is not finance specific, the volume of financial information exchanged on Twitter is substantially larger than on bespoke finance platforms.
<!-- 金融に特化しているわけではないものの、Twitterで交換される金融情報の量は専門の金融プラットフォームよりもはるかに多い。 -->
- In addition, Twitter is more likely to report the market, competitor and product specific problems that tend to drive valuations today.
<!-- 加えて、Twitterは、今日の評価を左右しそうな市場や競合他社、製品に固有の問題を報告する可能性が高い。 -->

## `rtweet` package

- I use the rtweet package here to extract information from Twitter.
<!-- 本書では，`rtweet`パッケージを使用して，Twitterから情報を抽出する。 -->
- The `#` symbol is used to refer to individuals on Twitter.
<!-- `#`記号は、Twitter上の個人を参照するために使用される。 -->
- A `#` symbol before a word tells Twitter to “index” all transactions that have this hashtag.
<!-- 単語の前に`#`記号があると、Twitterはこのハッシュタグを持つすべてのトランザクションを「インデックス」する。 -->
- Twitter accumulates all posts that have it and retrieves them more quickly than would search with a query engine.
<!-- Twitterはハッシュタグをもつすべての投稿を蓄積し、それらを検索エンジンで検索するよりも迅速に取得する。 -->


## Twitter

- The `@` symbol is used to refer to individuals on Twitter.
<!-- `@`記号は、Twitter上の個人を参照するために使用される。 -->
- It is combined with a username and inserted into tweets to refer to that person or send them a public message.
<!-- ユーザ名と組み合わせて、その人を参照したり、公開メッセージを送ったりするためにツイートに挿入される。 -->
- When `@` precedes a username, it automatically gets linked to that user’s profile page.
<!-- `@`がユーザ名の前にあると、自動的にそのユーザのプロフィールページにリンクされる。 -->
- Users may be individuals or firms.
<!-- ユーザは個人または企業である。 -->

---

- It is not necessary to receive a token to use rtweet; only a user account is required.
<!-- `rtweet`パッケージを使う際に必要なのはユーザアカウントだけで，トークンを受け取る必要はない。 -->
- But the OAuth authentication gives access to more functions on Twitter, such as posting.
<!-- しかし，OAuth認証を行うと，Twitter上での投稿などの機能にアクセスできるようになる。 -->
- If you need full access, go to https://developer.twitter.com (Twitter Developer Site) create an app and apply for a consumer key and consumer secret; as the “Callback URL” enter: http://127.0.0.1:1410.
<!-- 完全なアクセスが必要な場合は、https://developer.twitter.com（Twitter Developer Site）にアクセスし、アプリを作成し、コンシューマキーとコンシューマシークレットを申請します。 -->
 <!-- 「コールバックURL」には、http://127.0.0.1:1410を入力する。 -->


---

- The authentication process of Twitter involves creating a Twitter app and doing a handshake.
<!-- Twitterの認証プロセスは、Twitterアプリを作成し、ハンドシェイクを行うことを含む。 -->
- You have to handshake every time you want to get data from Twitter with R.
<!-- RでTwitterから情報を取得したいときは，毎回ハンドシェイクが必要になる。 -->
- Since Twitter released the Version 1.1 of their API an OAuth handshake is necessary for every request you do.
<!-- TwitterはAPIのバージョン1.1をリリースしたため、リクエストごとにOAuthハンドシェイクが必要になる。 -->


---

The following steps will get you onto the Twitter API:
<!-- 以下の手順でTwitter APIにアクセスできる。 -->

1. Create an app at Twitter: Go to apps.twitter.com/ and log in with your Twitter Account.
   From your Profile picture in the upper right corner, select the drop-down menu and look for “My Applications”.
  Click on it and then on “Create new application”.
  As the Callback URL enter * http://127.0.0.1:1410.
  Click on Create and you will get redirected to a screen with all the OAuth settings of your new App.
1. Use the `setup_twitter_OAuth()` function which uses the httr package.
   Get your api_key and your `api_secret` as well as your `access_token` and `access_token_secret` from your app settings on Twitter (click on the “API key” tab to see them).

:::{.notes}
1 Twitterアプリを作成する：apps.twitter.com/にアクセスし、Twitterアカウントでログインする。
右上隅のプロフィール画像から、ドロップダウンメニューを選択し、「My Applications」を探す。
クリックしてから「Create new application」をクリックする。
コールバックURLとして* http://127.0.0.1:1410.を入力する。
クリックして、新しいアプリのOAuth設定が表示される画面にリダイレクトされる。
2 `setup_twitter_OAuth()`関数を使用する。httrパッケージを使用する。
Twitterのアプリ設定から`api_key`、`api_secret`、`access_token`、`access_token_secret`を取得する（これらを表示するには「APIキー」タブをクリックする）。
:::


## Example: Extracting Tweets about R

Here is an example.
<!-- これが例である。 -->


```{r rtweet}
#| eval: false

## トークンを作成し、環境変数として保存する
create_token(
  app = "Test_of_the_API_platform", # appの名前
  consumer_key = 'AWsZc3pjFsgAFlBK4OHRlyGtK', # コンシューマキー
  consumer_secret = 'DTRvorcjSaQQ1goWzynZ2tc226mgRvQ1JPxGur7nQMTesuXw3z', # シークレットキー
  access_token = '14122740-FWlOwlo4qvhiy6oTcRypgVaIyvmlg1OZLudAToO6c', # アクセストークン
  access_secret = 'sYjzQMjFKQFvMVRCU9gYx7bOteiS4XCoLvCgodTJZVm7y' # アクセスシークレット
  )

## Google Mapsを通じて地理位置データにアクセスするためのGoogle APIキー
westland_api <- 'AIzaSyCErk3aBmPoG1FAKEqNUz6elhD6ZrR2MQtN7W0'

rt <- search_tweets("#rstats", n = 18000, include_rts = FALSE)

# うごかない
rt <- search_tweets(
  "#rstats",
  n = 18000,
  include_rts = FALSE
) # うごかない
```


## Example: Extracting tweets about General Motors and the Auto Industry

Query used to select and customize streaming collection method.
There are four possible methods.

1. The default, `q = ""`, returns a small random sample of all publicly available Twitter statuses.
2. To filter by keyword, provide a comma separated character string with the desired phrase(s) and keyword(s).
3. Track users by providing a comma separated list of user IDs or screen names.
4. Use four latitude/longitude bounding box points to stream by geo location.

<!-- This must be provided via a vector of length 4, e.g., `c(-125, 26, -65, 49)`. -->

:::{.notes}
ストリーミング収集方法を選択してカスタマイズするために使用されるクエリ。
4つの可能な方法がある。
1 デフォルトの`q = ""`は、すべての公開可能なTwitterステータスの小さなランダムサンプルを返す。
2 キーワードでフィルタリングするには、希望のフレーズとキーワードを含むカンマ区切りの文字列を提供する。
3 ユーザIDまたはスクリーン名のカンマ区切りリストを提供することで、ユーザをトラックする。
4 緯度/経度の境界ボックスの4つのポイントを使用して、ジオロケーションでストリーミングする。
:::


---

```{r rtweet_GM}
#| eval: false
stream_tweets(
  q = "auto, car, general motors, GM", # キーワードでフィルタリング
  timeout = 100,    ## the number of seconds that you will access.
                    ## Max 18000 tweets / 15 min
  parse = FALSE,    ## we'll do this later
  file_name = "tweetsaboutGM.json"
) # うごかない

## read in the data as a tidy tbl data frame
djt <- parse_stream("tweetsaboutGM.json")

djt <- djt[,3:6]  ## just a few things we'd like to see
glimpse(djt)

# To get an idea of what you should be seeing before you actually signup for a Twitter OAuth code,
# you can bring in the dataset *trump_tweet.RData # provided with this workout
```





## Intelligence Scanning of Curated News Streams

- Curated intelligence sources, such as Google News, MacRumours, and other news feeds, offer prepackaged information that can be.
<!-- Google News、MacRumours、その他のニュースフィードなどのキュレーションされた情報ソースは、事前にパッケージ化された情報を提供している。 -->
- My favorite is Feedly (stylized as feedly), a news aggregator application that compiles news feeds from a variety of online sources for the user to customize and share with others.
<!-- 私のお気に入りはFeedly（feedlyとしてスタイライズされている）で，ユーザーがカスタマイズして他のユーザと共有するために、さまざまなオンラインソースからニュースフィードをまとめるニュースアグリゲーターアプリケーションです。 -->
- Feedly is emblematic of the sort of cloud-based information services that are revolutionizing the audit process.
<!-- Feedlyは、監査プロセスを革新しているクラウドベースの情報サービスの代表的なものである。 -->

---

- Start by going to [https://developer.feedly.com/](https://developer.feedly.com/) and then page to [https://feedly.com/v3/auth/dev](https://feedly.com/v3/auth/dev) to sign in, get a user ID and then follow steps to get your access token.
<!-- まず、https://developer.feedly.com/にアクセスし、https://feedly.com/v3/auth/devに移動してサインインし、ユーザーIDを取得し、手順にしたがってアクセストークンを取得する。 -->
- This requires either a "Pro" account or a regular account and you manually requesting OAuth codes.
<!-- これには「Pro」アカウントまたは通常のアカウントが必要であり、OAuthコードを手動でリクエストする必要がある。 -->
- Store it in your `~/.Renviron` in `FEEDLY_ACCESS_TOKEN`.
<!-- これを`~/.Renviron`に`FEEDLY_ACCESS_TOKEN`として保存する。 -->
- Feedly will return your access token and refresh token, which looks like the tokens below, and which you can save on your computer, leaving you free to easily access Feedly content.
<!-- Feedlyは、以下のようなトークンを返し、コンピュータに保存することができる。 -->
<!-- これにより、Feedlyのコンテンツに簡単にアクセスできるようになる。 -->


---

```{r}
#| eval: false
pkgenv <- new.env(parent = emptyenv()) # 新しい環境を作成
pkgenv$token <- Sys.getenv("FEEDLY_ACCESS_TOKEN") # 環境変数からトークンを取得
#’ In reality, this is more complex since the non-toy example has to #’ refresh tokens when they expire.
# 実際には、トークンが期限切れになったときにトークンを更新する必要があるため、より複雑になる。
feedly_token <- function() {
    return(.pkgenv$token)
}
```

---

- For the purposes of this example, consider a “stream” to be all the historical items in a feed.
<!-- この例における目的は，「ストリーム」とはフィード内のすべての過去のアイテムを指すものとする。 -->
- Maximum "page size" (max number of items returned in a single call) is 1000.

- For simplicity, there is a blanket assumption that if continuation is actually present, we can ask for a large number of items (e.g., 10,000).

---


```{r}
#| eval: false

apple_feed_id <- "feed/http://feeds.feedburner.com/MacRumors"
## Here is the stream function
feedly_stream(stream_id,
ranked = c("newest", "oldest"),
unread_only = FALSE,
count = 1000L,
continuation = NULL,
feedly_token = feedly_access_token())
apple_stream <- feedly_stream(apple_feed_id) glimpse(apple_stream)
## Here is another function
feedly_search_contents(query,
stream_id = NULL,
fields = "all",
embedded = NULL,
engagement = NULL,
count = 20L,
locale = NULL,
feedly_token = feedly_access_token())
f_search <-
feedly_search_contents(q = "ipod",
glimpse(f_search)
# preallocate space
streams <- vector("list", 10) streams[1L] <- list(apple_stream)
# catch all of the content
idx <- 2L while(length(apple_stream$continuation) > 0) {
cat(".", sep="") # progress bar, sort of feedly_stream(
stream_id = apple_feed_id,
ct = 10000L,
continuation = apple_stream$continuation
) -> rb_stream
streams[idx] <- list(apple_stream) idx <- idx + 1L
} cat("\n")
str(streams, 1) str(streams[[1]], 1) glimpse(streams[[1]]$items)
```


---

- Feedly curates numerous news and blog outlets, which distribute a broad array of news items from formal press publications that would have a bearing on the conduct of an audit.
<!-- Feedlyは、監査の実施に影響を与える可能性のある公式の報道出版物から、多くのニュースやブログのアウトレットをキュレーションし、幅広いニュースアイテムを配信している。 -->
- I suggest you use the various `lubridate`, `ggplot`, and `tidyverse` tools to analyze and present any insights from this larger dataset.
<!-- このような大規模なデータセットからの洞察を分析し、提示するために、`lubridate`、`ggplot`、`tidyverse`のさまざまなツールを使用することをお勧めします。 -->

---


- There are many sources of curated news for conducting analytical reviews in an audit, each with its own merits.
<!-- 監査における分析的検討のためのキュレーションされたニュースは多く存在し、それぞれに長所がある。 -->
- The largest consolidator of curated news on the web is arguably Google.
<!-- ウェブ上で最も大きなキュレーションされたニュースの集約者は、おそらくGoogleである。 -->
- The `newsAPI` package is used to access Google’s News API using R.
<!-- `newsAPI`パッケージは、Rを使用してGoogleのニュースAPIにアクセスするために使用される。 -->

---

```{r newsAPI}
#| eval: false

# go to newsapi.org and register to get an API key.
# save the key as an environment variable
## my obscured key
NEWSAPI_KEY <- "079ee9e373894dcfb9a062a85c4e1e7e"
## save to .Renviron file
cat(
paste0("NEWSAPI_KEY=", NEWSAPI_KEY), append = TRUE,
fill = TRUE,
file = file.path("~", ".Renviron")
)
src <- get_sources(
  category = "", language = "en", country = "",
  apiKey = NEWSAPI_KEY, parse = TRUE)
## load package

df <- lapply(src$id, get_articles,apiKey=NEWSAPI_KEY)
## collapse into single data frame
df <- do.call("rbind", df)
```

<!--
## additional functions allow the parsing of streamed news articles
# get_sources(
  #category = "", language = "", country = "", apiKey = NEWSAPI_KEY,
# x <- get_sources(
  #category = "", language = "", country = "", apiKey = NEWSAPI_KEY,
#parse_sources(x)
# y <- get_articles(
      # source, sortBy = "top", apiKey = NEWSAPI_KEY, parse = TRUE)
 #parse_articles(y)
 # source:Name of news source.
 # sortBy: Name of sorting mechanism must be one of
    ## latest, top, or popular. Certain methods only work for certain news sources.
 # apiKey: Character string API token. Default is to grab it from user R environ.
 # parse: Logical indicating whether to parse response object to data frame.
  # where x is a response object from get_sources
 -->

---

- Feedly is a news aggregator application for various web browsers and mobile devices running "iOS" and "Android", also available as a cloud-based service.
- It compiles news feeds from a variety of online sources for the user to customize and share with others.
- Methods are provided to retrieve information about and contents of "Feedly" collections and streams.

:::{.notes}
Feedlyは、さまざまなWebブラウザや「iOS」、「Android」を実行するモバイルデバイス向けのニュースアグリゲータアプリケーションであり、クラウドベースのサービスとしても利用できる。
さまざまなオンライン・ソースからのニュース・フィードを編集し、ユーザーがカスタマイズして他のユーザーと共有できる。
「Feedly」コレクションやストリームに関する情報やコンテンツを取得するためのメソッドが提供されている。
:::

---

- Neither `feedly_search()` nor `feedly_stream()` require authentication (i.e., you do not need a developer token) to retrieve the contents of the API call.
<!-- `feedly_search()`も`feedly_stream()`も、APIコールの内容を取得するのに認証は不要である (つまり、開発者トークンは不要)。 -->
- For `feedly_stream()`, you do need to know the Feedly-structured feed id which is (generally) feed/FEED_URL (e.g., feed / http://feeds.feedburner.com/RBloggers).
<!-- `feedly_stream()`については、Feedlyで構造化されたfeed IDが必要であり，(一般的に) feed/FEED_URL (例: feed/http://feeds.feedburner.com/RBloggers) となっている。 -->
- I have generally found Feedly to be more useful than Google News for business intelligence scanning, because it is less hampered by throttling, and the curation extends to industry specific feeds (rather than Google’s algorithmic guess about the topic).
<!-- ビジネス・インテリジェンスのスキャンには、Google NewsよりもFeedlyの方が便利である。というのもFeedlyはスロットリングに邪魔されることが少なく、キュレーションが（トピックに関するGoogleのアルゴリズムによる推測ではなく）産業固有のフィードにまで及ぶからである。- スロットリング（throttling）とは、システムの過負荷や特定利用者による資源の独占を回避するため、一定の制限値を超えた場合に意図的に性能を低下させたり、要求を一時的に拒否したりする制御のこと。- -->

---

- In the following example, consider a “stream” to be all the historical items in a feed.
- Maximum "page size" (max number of items returned in a single call) is 1000.
<!-- 最大「ページサイズ」（1回の呼び出しで返されるアイテムの最大数）は1000である。 -->
- For simplicity, there is a blanket assumption that if continuation is actually present, we can ask for a large number of items (e.g., 10,000).
<!-- 単純化のために，継続が実際に存在する場合，多くのアイテム（例えば，10000個）を要求することができるというblanket assumptionがある。？？ -->

## Example: Extracting News about Apple

```{r}
read.csv(system.file("extdata",
    "feedly_functions.csv",
    package = "auditanalytics", mustWork = TRUE)) |>
    kable() |> kable_styling(font_size = 12)
```



## Example: feedly

<!--
# Find writeups for seymour functions at https://github.com/hrbrmstr/seymour/tree/master/man
## useful functions for extracting news are:
# feedly_search_contents: Search content of a stream
# feedly_search_title: Find feeds based on title, url or ‘#topic’
# feedly_stream: Retrieve contents of a Feedly ‘‘stream’’
# feedly_subscribe: Subscribe to an RSS feed
# feedly_subscriptions: Retrieve Feedly Subscriptions
## Let’s check what is happening at Apple
## (search Google’s "feedburner.com" and then the corporation names to see what is available)
## the following should retreive your Feedly access token, but if not,
## you can log into the site and request the token.
 -->


```{r}
#| eval: false

token <- feedly_access_token()
feedly_search_title("roland")
# prefix the URL with ’feed/’
music_feed_id <- "feed/http://feeds.feedburner.com/MusicRadar"
music_feed_id_2 <- "feed/http://feeds.feedburner.com/MusicTech"
## Here is the stream function
feedly_stream(stream_id, unt = 1000L,
continuation = NULL,
feedly_token = feedly_access_token())
music_stream <- feedly_stream(music_feed_id_2) glimpse(music_stream)

## Here is another function
feedly_search_contents(query,
  stream_id = NULL,
  fields = "all",
  embedded = NULL,
  engagement = NULL,
  count = 20L,
  locale = NULL,
  feedly_token = feedly_access_token()
  )

f_search <- feedly_search_contents(
  q = "ipod",
  stream_id = "music_feed_id",
  fields = "keywords")
glimpse(f_search)

# preallocate space
streams <- vector("list", 10)
streams[1L] <- list(music_stream)

# catch all of the content
idx <- 2L
while(length(music_stream$continuation) > 0) {
  cat(".", sep="") # progress bar, sort of
  feedly_stream(
    stream_id = music_feed_id,
    ct = 10000L,
    continuation = music_stream$continuation
  ) -> rb_stream
  streams[idx] <- list(music_stream)
  idx <- idx + 1L
}
cat("\n")

str(streams, 1)
str(streams[[1]], 1)
glimpse(streams[[1]]$items)
```

---

- <mark class="mark">API datastreams from social networks, blogs, and other Internet resources tend to be best for qualitative intelligence scanning</mark>.
- They can alert the auditor to information that would not appear in financial news or in the accounting statements and transactions.
- <mark class="mark">Such information is an essential part of the analytical review process</mark>, but until the advent of Internet accessible resources and automated tools provided by _R_, has not been accessible to auditors in a cost-effective way.

:::{.notes}
APIデータストリームは、ソーシャルネットワーク、ブログ、その他のインターネットリソースからのデータは、質的な情報スキャンに最適である。
APIデータストリームは、財務ニュースや会計報告書、取引には現れない情報を監査人に知らせることができる。
このような情報は、分析的検討(analytical review process)において不可欠なものだが、インターネットからアクセス可能なリソースやRが提供する自動化ツールが登場するまでは、監査人が費用対効果の高い方法で情報にアクセスすることはできなかった。
:::


# Accessing General Web Content through Web Scraping

## Introduction to Web Scraping

- Where relevant intelligence is not available through APIs, but is <mark class="mark">presented on websites, it is possible to web scrape data</mark>.
<!-- APIで利用できなくても、ウェブサイトで情報が提供されている場合、ウェブスクレイピングでデータを取得することができる。 -->
- This can be difficult and messy, but <mark class="mark">R provides a number of effective helper tools</mark> to scrape and organize data from websites.
<!-- この作業は難しく，ごちゃごちゃになることもあるが，Rにはウェブサイトからデータをスクレイピングして整理するための効果的なヘルパーツールがいくつか用意されている。 -->


---


- I provide here a brief introduction to the concept and practices of web scraping in R using the `rvest` package.

- Tools like `rvest` and **Beautiful Soup** (Python) inject structure into **web scraping**, which has become important because so few companies are willing to part with their proprietary customer datasets.

:::{.notes}
ここでは、`rvest`パッケージを使ってRでウェブスクレイピングの概念と実践について簡単に紹介する。
`rvest`やPythonの`Beautiful Soup`といったツールは、ウェブスクレイピングに構造を注入するために重要になってきている。というのも、独自の顧客データを手放そうとする企業が少ないためである。
:::


---


- They have no choice but to expose some of this proprietary data via the web, though, and this is where auditors have an opportunity to accumulate valuable information germane to audit risk.
- The process of scraping data from the web exemplifies the **computer-plus-human model** of computing.
- It is also a nice introduction to building custom software for scraping a specific website.

:::{.notes}
しかし、企業はウェブサイトを通じてその独自のデータの一部を公開せざるをえず，これが監査リスクに関連する貴重な情報を蓄積する機会を監査人に与えている。
ウェブからデータをスクレイピングするプロセスは、コンピューティングのコンピュータ＋人間モデルを体現している。
また、特定のウェブサイトをスクレイピングするためのカスタム・ソフトウェアを構築する良い導入でもある。
:::


## `rvest` Basics

The basic functions in `rvest` are powerful, and you should try to utilize the following functions when starting out a new web scraping project.
<!-- `rvest`の基本関数は強力であり、新しいウェブスクレイピングのプロジェクトを開始する際には、以下の関数を利用することをお勧めする。 -->

- `html_nodes():`  identifies HTML wrappers.
<!-- - `html_nodes():` HTMLラッパーを識別する。 -->
- `html_nodes(".class"):`  calls node based on css class
<!-- - `html_nodes(".class"):`  cssクラスに基づいてノードを呼び出す。 -->
- `html_nodes("#id"):`  calls node based on id
<!-- - `html_nodes("#id"):` idに基づいてノードを呼び出す。 -->
- `html_nodes(xpath="xpath"):`  calls node based on xpath
<!-- - `html_nodes(xpath="xpath"):` xpathに基づいてノードを呼び出す。 -->

---

<!-- - `html_attrs():`  identifies attributes (useful for debugging) -->
- `html_attrs():` 属性を識別する（デバッグに便利）。
<!-- - `html_table():`  turns HTML tables into data frames -->
- `html_table():`  HTMLテーブルをデータフレームに変換する。
<!-- - `html_text():`  strips the HTML tags and extracts only the text -->
- `html_text():`  HTMLタグを剥がし、テキストのみを抽出する。

**Note on plurals:** html_node() returns metadata; but html_nodes() iterates over the matching nodes.
<!-- **複数形についての注意:** `html_node()`はメタデータを返すが、`html_nodes()`は一致するノードを反復処理する。 -->
The html_nodes() function turns each HTML tag into a row in an R dataframe.
<!-- `html_nodes()`関数は、各HTMLタグをRデータフレームの行に変換する。 -->


## SelectorGadget

- `SelectorGadget` is a javascript bookmarklet that allows you to interactively figure out what css selector you need to extract desired components from a page.
<!-- SelectorGadgetは、あなたがページから望むコンポーネントを抽出するために必要なcssセレクタを対話的に見つけることができるjavascriptブックマークレットである。 -->
<!-- To install it, go to the page: https://cran.r-project.org/web/packages/rvest/vignettes/selectorgadget.html -->
<!-- SelectorGadgetをインストールするには，次のウェブサイトにアクセスする。 -->
<!-- [https://cran.r-project.org/web/packages/rvest/vignettes/selectorgadget.html](https://cran.r-project.org/web/packages/rvest/vignettes/selectorgadget.html) -->


- Install `selectorgadget` on the Chrome Browser (only at the time of this writing) from `https://selectorgadget.com/`.
<!-- Chromeブラウザにselectorgadgetをインストールするには，（この執筆時点では）[https://selectorgadget.com/](https://selectorgadget.com/)からインストールする。 -->
`SelectorGadget` is an open-source tool that simplifies CSS selector generation and discovery on complicated sites.
<!-- SelectorGadgetは、複雑なサイトでのCSSセレクタの生成と発見を簡素化するオープンソースのツールである。 -->


## How to use SelectorGadget

- Install the Chrome Extension or drag the bookmarklet to your bookmark bar, then go to any page and launch it.
- A box will open in the bottom right of the website.
<!-- Chrome拡張機能をインストールするか、ブックマークレットをブックマークバーにドラッグして、任意のページに移動してSelectorGadgetを起動させと，ウェブサイトの右下にボックスが開く。 -->
- Click on a page element that you would like your selector to match (it will turn green).
<!-- あるページの要素をクリックすると、その要素と一致するセレクタが生成される（緑色になる）。 -->

- `SelectorGadget` will then generate a minimal CSS selector for that element, and will highlight (yellow) everything that is matched by the selector.
<!-- SelectorGadgetは、その要素に対する最小のCSSセレクタを生成し、そのセレクタに一致するすべてのものを黄色くハイライトする。 -->
- Now click on a highlighted element to remove it from the selector (red), or click on an unhighlighted element to add it to the selector.
<!-- 次に、セレクタからハイライトされた要素を削除するためにクリックする（赤色になる）か、ハイライトされていない要素をクリックしてセレクタに追加する。 -->
<!-- - Through this process of selection and rejection, SelectorGadget helps you come up with the perfect CSS selector for your needs. -->
<!-- この選択と拒否のプロセスを通じて、SelectorGadgetはあなたのニーズに合った完璧なCSSセレクタを見つけることに役立つ。 -->

## How to use SelectorGadget 2

To use it, open the page:
<!-- ウェブサイトを開いて使ってみよう。 -->

1. Click on the element you want to select.
<!-- 1. 選択したい要素をクリックする。 -->
`Selectorgadget` will make a first guess at what css selector you want.
<!-- Selectorgadgetは、あなたが望むcssセレクタを最初に推測する。 -->
It is likely to be bad since it only has one example to learn from, but it is a start.
<!-- それは1つの例しか学習していないのでおそらく悪いものになるが、最初だけである。 -->
Elements that match the selector will be highlighted in yellow.
<!-- セレクタに一致する要素は黄色でハイライトされる。 -->
1. Click on elements that should not be selected.
They will turn red.
<!-- 3. 選択されてはいけない要素をクリックすると赤くなる。 -->
Click on elements that should be selected.
They will turn green.
<!-- 選択されるべき要素をクリックすると，緑色になる。 -->
1. Iterate until only the elements you want are selected.
<!-- 3. 望む要素だけが選択されるまで繰り返す。 -->
Selectorgadget is not perfect and sometimes will not be able to find a useful css selector.
Sometimes starting from a different element helps.
<!-- Selectorgadgetは完璧ではなく、時々有用なcssセレクタを見つけることができないことがあるが，ときおり異なる要素から始めることが役立つことがある。 -->


## How to use SelectorGadget 3
<!-- ## 他の重要な関数 -->

1. If you prefer, you can use xpath selectors instead of `css: html_nodes(doc, xpath = "//table//td")`.
<!-- 1. 望むなら、cssの代わりにxpathセレクタを使うこともできる: `html_nodes(doc, xpath = "//table//td"))`. -->
2. Extract the tag names with `html_tag()`, text with `html_text()`, a single attribute with `html_attr()` or all attributes with `html_attrs()`.
<!-- 1. `html_tag()`でタグ名を、`html_text()`でテキストを、`html_attr()`で単一の属性を、`html_attrs()`で全ての属性を抽出する。 -->
3. Detect and repair text encoding problems with guess_encoding() and repair_encoding().
<!-- 3. `guess_encoding()` および`repair_encoding()`で、テキスト・エンコーディングの問題を検出して修復する。 -->
4. Navigate around a website as if you are in a browser with `html_session()`, `jump_to()`, `follow_link()`, `back()`, and `forward()`.
Extract, modify, and submit forms with `html_form()`, `set_values()`, and `submit_form()`.
<!-- 1. `html_session()`、`jump_to()`、`follow_link()`，`back()`， `forward()`を使って、あたかもウェブブラウザの中にいるかのようにウェブサイトを移動し，`html_form()`、`set_values()`、`submit_form()`を使用して、フォームの抽出、修正、送信を行う。 -->

## Simple Sentiment Analysis
<!-- ## シンプルな感情分析 -->

- Here is an example of a simple sentiment analysis for customers’ comments on restaurants in Hanoi Vietnam.
<!-- 以下は、ベトナムのハノイのレストランに関する顧客のコメントに対するシンプルな感情分析の例である。 -->
- Start by pointing your browser to https://www.tripadvisor.com and searching for “Asian” cuisine in “Hanoi”.
<!-- - (as homework, consider other cities or services based on specific audit needs). -->
<!-- まず、ウェブブラウザで[https://www.tripadvisor.com](https://www.tripadvisor.com)を訪れ、"Hanoi"の"Asian"料理を検索する（宿題として、特定の監査ニーズ(audit needs)に基づいて他の都市やサービスを考えてみよう）。 -->
- Click on the “Asian” menu, which brings you to web page.
<!-- https://www.tripadvisor.com/Restaurants-g293924-Hanoi.html. -->
<!-- "Asian"メニューをクリックすると、[https://www.tripadvisor.com/Restaurants-g293924-Hanoi.html](https://www.tripadvisor.com/Restaurants-g293924-Hanoi.html)というウェブページに移動する。 -->
- Turn on `selectorgadget` in Chrome browser and highlight all of the reviews.
<!-- Chromeブラウザでselectorgadgetをオンにして、すべてのレビューをハイライトしよう。 -->

## selectorgadget

- In the menu at the bottom of your screen, this will give you an index ".is-9" which is the designator for the CSS code that you have outlined (you can verify this in Chrome by clicking the three dot menu at the upper right-hand corner of the screen,
- clicking “More Tools” = “Developer Tools” and
- checking the webpage HTML; or right-click and inspect for a quick look)
<!-- 画面下部のメニューに".is-9 "というインデックスが表示される。 -->
<!-- これは、アウトライン化したCSSコードを示している（Chromeでは、画面右上の3つのドットメニューをクリックし、"その他のツール"="デベロッパーツール "をクリックし、ウェブペ
ージのHTMLを確認することで確認できる。） -->


## Example: Trip Advisor Reviews

```{r}
#| cache: true
#| output-location: slide
url <- "HTML/hanoi.html" # htmlファイルの場所
reviews <- url |> read_html() |> # rvest
    html_nodes("._T") # selectorgadget
## Pull the text out of the reviews
quote <- reviews |>  html_text()
## Turn the character string "quote" into a data.frame and View
## data.frame(quote, stringsAsFactors = FALSE) |> View() # 中身チェック
pal2 <- brewer.pal(8,"Dark2") ## from RColorBrewer
wordcloud(quote, colors = pal2)
```

- In TripAdvisor, you can use the same methods, in various geographical regions, for: Hotels, Things to do, Restaurants, Flights, Vacation Rentals,Cruises and other things.
<!-- TripAdvisorでは、ホテル、アクティビティ、レストラン、フライト、バケーションレンタル、クルーズなど、さまざまな地理的地域で同じ方法を使用できる。 -->
- Similar methods work for other review and aggregation sites.
<!-- 他のレビューや集計サイトでも同様の方法が使える。 -->






## Example: Movie Reviews

- The next example scrapes information about "The Lego Movie" from IMDB.
<!-- 次の例は，IMDBから「レゴ・ムービー」に関する情報をスクレイピングする。 -->
- We start by downloading and parsing the file with `html()`.
<!-- まず、`html()`を使ってファイルをダウンロードして解析する。 -->
- To extract the rating, we start with `Selectorgadget` to figure out which css selector matches the data we want.
<!-- 評価を抽出するためには、Selectorgadgetを使って、どのcssセレクタが欲しいデータと一致しているのかを探す。 -->
- We use `html_node()` to find the first node that matches that selector, extract its contents with `html_text()`, and convert it to numeric with `as.numeric()`.
<!-- `html_node()`関数を用いて、そのセレクタに一致する最初のノードを見つけ、`html_text()`でその内容を抽出し、`as.numeric()`で数値に変換する。 -->

## Example: Tabular Data

- Some websites publish their data in an **easy-to-read table** without offering the option to download the data.
<!-- いくつかのウェブサイトは、データをダウンロードするオプションを提供せずに、読みやすい表形式で公開されている。 -->
- Package `rvest` uses `html_table()` for tabular data.
<!-- `rvest`パッケージは，表形式のデータに対して`html_table()`を使う。 -->
- Using the functions listed above, isolate the table on the page.  Then pass the HTML table to `html_table()`.
<!-- 上記の関数を使って、ページ上の表を抽出し，次にHTMLテーブルを`html_table()`に渡す。 -->
- In the following case, you can go to https://www.nis.gov.kh/cpi/ and inspect the html.
<!-- 次の例では、[https://www.nis.gov.kh/cpi/](https://www.nis.gov.kh/cpi/)にアクセスして、htmlを調査することができる。 -->

---

```{r}
#| code-fold: true
accounts <- read_html("https://www.nis.gov.kh/cpi/Apr14.html")

table <- accounts %>%
  html_nodes("table") %>%
  html_table(header=T)

# 表を整理する
# 1番目のテーブルを表示
dict <- table[[1]][,1:2] # 表1の1〜2列目を抽出
accounts_df <- table[[1]][6:18,-1] # 表1の6〜18行目を抽出

# 列名を指定
names <- c('id', 'weight.pct', 'jan.2013', 'dec.2013', 'jan.2014', 'mo.pctch', 'yr.pctch', 'mo.cont', 'yr.cont')
colnames(accounts_df) <- names # データフレームの列名を変更

glimpse(accounts_df) # データの確認
```


## Example: XPaths

- `Xpaths` are content hierarchies in a website.
<!-- Xpathsはウェブサイトのコンテンツ階層である。 -->
- Sometimes you can get more comprehensive retrieval with an `xpath`.
<!-- xPathを使うと、より包括的な取得ができることがある。 -->
- You can get the `xpath` that includes some content with the Chrome xPath Finder extension (it is like `SelectorGadget` but for xpaths)
<!-- Chrome xPath Finder拡張機能を使うと、xPathを取得できる（これはSelectorGadgetのようなものだが、xPath用である）。 -->

## Example: Extracting Data from Wikipedia

```{r}
#| output-location: slide
h <- read_html("https://en.wikipedia.org/wiki/Current_members_of_the_United_States_House_of_Representatives")
reps <- h |>
    html_node(xpath = '//*[@id="votingmembers"]') |>
    html_table(fill = T)
reps[, c(1:2, 4:9)] |> as_tibble() |>
  head(10) |> kable() |> kable_styling(font_size = 12)
```


## Example: Extracting Intelligence from Product User Forums

- Product user forums are excellent sources of informed consumer and retailer information.
<!-- 製品ユーザーフォーラムは、情報に詳しい消費者や小売業者の情報源として優れている。 -->
- This example provides a number of methods that can be used for general web scraping.
<!-- この例は、一般的なウェブスクレイピングに使用できるいくつかの方法を提供する。 -->
- Applying this to our goal of web scraping for intelligence on Roland's products, we can glean consumer sentiment on Roland's pianos as conveyed by discussions on the Piano World Forum website and display it with `wordcloud`.
<!-- ローランドの製品に関する情報をウェブスクレイピングする，という目標を設定し、Piano World Forumのウェブサイトでの議論からローランドのピアノに対する消費者の感情を把握し、 -->
<!-- `wordcloud`で表示することができる。 -->


## Word Cloud of Roland Piano Reviews

```{r pianoworld}
#| cache: true
#| code-fold: true
handle <- handle("http://forum.pianoworld.com//") # handle to the website
# path to the login page
path <- "ubbthreads.php/ubb/login.html?ocu=http%3A%2F%2Fforum.pianoworld.com%2F"

# fields found in the login form.
login <- list(
  amember_login = "westland", # ユーザーネーム
  amember_pass  = "powerpcc", # パスワード
  amember_redirect_url = "http://forum.pianoworld.com//ubbthreads.php/forum_summary.html"
  )
response <- POST(handle = handle, path = path, body = login)

# Copy the URL of the page you are scraping
url <- "http://forum.pianoworld.com/"

# Extract the reviews in the CSS selector
reviews <- url %>%
           read_html() %>%
           html_nodes("#cat2 div")

# Pull the selected text out of the reviews
quote <- reviews %>%
                  html_text() %>%
                   as_tibble() # as_tibble()に変更

quote <-  filter(quote[str_detect(quote, "Roland")])

pal2 <- brewer.pal(8,"Dark2") # from RColorBrewer

wordcloud(unlist(quote), colors=pal2)
```

## Final Comments

- The previous algorithms and examples should give the reader an overview of some of the tools available to assist with analytical review.
<!-- 前述のアルゴリズムと例は、読者に分析検討(analytical review)を支援するためのいくつかのツールの概要を提供するはずである。 -->
<!-- - This is, of course, not a complete list of tools, or of opportunities. -->
<!-- もちろん，これは完全なツールのリストでも機会のリストでもない。 -->
- Much of the code in this chapter draws from <mark class="mark">dynamic and continually updated databases, streams, and standards</mark>.
<!-- この章の多くのコードは、動的で絶えず更新されるデータベースやデータ流(stream)、およびスタンダードから引用されている。 -->
- New tools and repositories are constantly coming online, particularly in accounting and finance, where the market is large and rich.
<!-- 特に会計と金融の分野では、市場が大きく豊かであるため、新しいツールやリポジトリが絶えずオンラインで登場している。 -->
- New packages are often introduced on **GitHub** rather than the official <mark class="mark">Comprehensive R Archive Network (CRAN)</mark> repositories, and auditors should always be on the lookout for new offerings.
<!-- 新しいパッケージは、公式のComprehensive R Archive Network (CRAN)リポジトリではなく、GitHubでよく紹介される。 -->
<!-- 監査人は常に新しいパッケージを探すことが望ましい。 -->

## Conclusion

- This chapter provides a start, but there are many opportunities for an ambitious auditor to develop new methodologies for plumbing Internet resources.
<!-- 本章はスタート地点を提供するが、野心的な監査人には、インターネットリソースを探るための新しい方法論を開発するための多くの機会がある。 -->
<!-- - This will require a level of patience and curiosity that exceeds merely writing code.  -->
<!-- これにはコードを書くこと以上の忍耐と好奇心が必要である。 -->
- As I mentioned in the preface, the R language is not a typical language with a single core of developers and guidelines; rather it is a <mark class="mark">sharing platform for a wide range of data analytic functions - features which make it useful, dynamic, and sometimes messy</mark>.
<!-- 前書きで述べたように、R言語は、単一の開発者のコアとガイドラインを持つ典型的な言語ではなく、幅広いデータ解析機能を共有するプラットフォームであり、それが有用で，動的で、時には混沌としている特徴を持っている。 -->
- The effort put into familiarizing oneself with the R ecosystem will pay off many times over in access to the latest, **most sophisticated algorithms** that exist anywhere in industry.
<!-- Rエコシステムに精通するための努力は、業界のどこにも存在する最新で最も洗練されたアルゴリズムに何度もアクセスすることで多くの利益をもたらすだろう。 -->
<!-- - Because of its unique character, you can be assured that R’s packages will always be at the forefront of financial analysis and analytical review, now and in the future. -->
<!-- その独特な性格のために、Rのパッケージは、今後も将来も常に金融分析と分析検討の最前線にあることを保証される。 -->
